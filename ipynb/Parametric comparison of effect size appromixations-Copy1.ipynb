{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will examine how well fit effect sizes compare to the emperical effect sizes for parametric data.\n",
    "\n",
    "1. Calculate the extrapolated power from the stimulation.\n",
    "2. Compare the effect size from the traditional power for the three methods of extrapolation.\n",
    "    * Look at the regression of all effects\n",
    "    * Look for values at tails (i.e. is there a maximum value for power which should be excluded?)\n",
    "    * Check the ratio between fitting methods\n",
    "3. Look at the effect size from emperical models for the three methods. Look for noise, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import scipy\n",
    "import statsmodels.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import emp_power.traditional as trad\n",
    "import emp_power.effects as eff\n",
    "import emp_power.plot as plot\n",
    "\n",
    "% matplotlib inline\n",
    "sn.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rounds = 100\n",
    "alpha = 0.05\n",
    "tests = ['ttest_ind']\n",
    "counts = np.arange(5, 100, 10)\n",
    "overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim_location = './simulations/'\n",
    "if not os.path.exists(sim_location):\n",
    "    raise ValueError('The simulations do not exist. Go back and simulate some data!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tests = ['ttest_ind', 'ttest_1', 'anova_3', 'anova_8', 'correlation']\n",
    "names = ['One sample T Test', 'Two Sample T Test', 'ANOVA (3 groups)', 'ANOVA (8 groups)', 'correlation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_power_summary(counts, power, alpha=0.05):\n",
    "    \"\"\"Calculates\"\"\"\n",
    "    # Calculates the effect size vectors\n",
    "    eff_z = eff.z_effect(counts, power, alpha=alpha)\n",
    "    eff_t = eff.t_effect(counts, power, alpha=alpha)\n",
    "    eff_f = eff.f_effect(counts, power, alpha=alpha)\n",
    "    \n",
    "    return (eff_z, eff_t, eff_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at effect sizes extrapolated from test-based methods. Since traditional power is a one-dimensional array and the effect size calculation returns a two-dimensional array, we will take the first observation in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trad_summary = {}\n",
    "for test_name in tests:\n",
    "    trad_summary[test_name] = {}\n",
    "    \n",
    "    # Checks the directory location\n",
    "    power_dir = os.path.join(sim_location, 'emperical_power/%s' % test_name)\n",
    "    trad_dir = os.path.join(sim_location, 'extrapolated_effect/test-based/%s' % test_name)\n",
    "    \n",
    "    # Creates the save directory\n",
    "    if not os.path.exists(trad_dir):\n",
    "        os.makedirs(trad_dir)\n",
    "        \n",
    "    # Looks at the simulations\n",
    "    for i in range(num_rounds):\n",
    "        power_fp = os.path.join(power_dir, 'simulation_%i.p' % i)\n",
    "        trad_fp = os.path.join(trad_dir, 'simulation_%i.p' % i)\n",
    "        \n",
    "        # Loads the power summary and adds it to the summary object\n",
    "        # Then skips to the next loop\n",
    "        if os.path.exists(trad_fp) and not overwrite:\n",
    "            with open(trad_fp, 'rb') as f_:\n",
    "                trad_summary[test_name][i] = pickle.load(f_)\n",
    "            continue\n",
    "\n",
    "        # Loads the power data\n",
    "        with open(power_fp, 'rb') as f_:\n",
    "            power_sim = pickle.load(f_)\n",
    "\n",
    "        # Draws the observations from the power simuation\n",
    "        power = power_sim['traditional_power']\n",
    "        counts = power_sim['counts']\n",
    "        \n",
    "        eff_z, eff_t, eff_f = calc_power_summary(counts, power)\n",
    "        \n",
    "        round_summary = {\n",
    "            'counts': counts,\n",
    "            'index': np.arange(len(counts)),\n",
    "            'power': power,\n",
    "            'dummy': i * np.ones(len(counts)).astype(int),\n",
    "            'f_effect': eff_f[0],\n",
    "            't_effect': eff_t[0],\n",
    "            'z_effect': eff_z[0],\n",
    "            }\n",
    "        trad_summary[test_name][i] = round_summary\n",
    "        \n",
    "        with open(trad_fp, 'wb') as f_:\n",
    "            pickle.dump(round_summary, f_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a summary dataframe, which will be easier to handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_traditional_values(test_name):\n",
    "    \"\"\"Generates a distance matrix summary for the analyses\"\"\"\n",
    "    test_data = pd.DataFrame(copy.copy(trad_summary[test_name])).transpose()\n",
    "    test_sum = pd.DataFrame(\n",
    "            data=[np.hstack(test_data['f_effect']),\n",
    "                  np.hstack(test_data['t_effect']),\n",
    "                  np.hstack(test_data['z_effect']),\n",
    "                  np.hstack(test_data['power']),\n",
    "                  np.hstack(test_data['dummy']),\n",
    "                  np.hstack(test_data['counts']),\n",
    "                  np.hstack(test_data['index']),\n",
    "                 ],\n",
    "            index=['f_effect', 't_effect', 'z_effect', 'power', \n",
    "                   'simulation', 'count', 'sim_position'],\n",
    "            ).transpose()\n",
    "    test_sum['test'] = test_name\n",
    "    test_sum['index'] = (test_sum['test'] + \n",
    "                         test_sum['simulation'].apply(lambda x: \".%i\" % x) + \n",
    "                         test_sum['count'].apply(lambda x: \".%02i\".zfill(2) % x)\n",
    "                         )\n",
    "    test_sum.set_index('index', inplace=True)\n",
    "    \n",
    "    nans = test_sum.index[pd.isnull(test_sum).any(1)]\n",
    "    \n",
    "    test_sum.drop(nans, inplace=True)\n",
    "    \n",
    "    return test_sum, nans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll combine all five tests to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 6)\n",
    "fig.set_size_inches(12, 10)\n",
    "f_all = []\n",
    "t_all = []\n",
    "z_all = []\n",
    "g_all = []\n",
    "pwr_all = []\n",
    "\n",
    "for (test_name, ax_r) in zip(*(tests, axes[1:])):\n",
    "#     print(test_name)\n",
    "    [ax1, ax2, ax3, ax4, ax5, ax6] = ax_r\n",
    "    data, nans = draw_traditional_values(test_name)\n",
    "    data.dropna(inplace=True)\n",
    "    f_ = data['f_effect']\n",
    "    t_ = data['t_effect']\n",
    "    z_ = data['z_effect']\n",
    "    g_ = data['count']\n",
    "    pwr = data['power']\n",
    "    \n",
    "    f_all.append(f_)\n",
    "    t_all.append(t_)\n",
    "    z_all.append(z_)\n",
    "    g_all.append(g_)\n",
    "    pwr_all.append(pwr)\n",
    "\n",
    "    plot.gradient_comparison(f_, t_, g_, pwr, [ax1, ax2], alpha=0.25, cmap='Spectral')\n",
    "    plot.gradient_comparison(z_, t_, g_, pwr, [ax3, ax4], alpha=0.25, cmap='Spectral')\n",
    "    plot.gradient_comparison(f_, z_, g_, pwr, [ax5, ax6], alpha=0.25, cmap='Spectral')\n",
    "    ax1.set_xlabel('')\n",
    "    ax3.set_xlabel('')\n",
    "    ax5.set_xlabel('')\n",
    "    ax1.set_ylabel(test_name)\n",
    "    ax3.set_ylabel('')\n",
    "    ax5.set_ylabel('')\n",
    "\n",
    "sn.despine()\n",
    "\n",
    "[ax1, ax2, ax3, ax4, ax5, ax6] = axes[0]\n",
    "f_all = np.hstack(f_all)\n",
    "t_all = np.hstack(t_all)\n",
    "z_all = np.hstack(z_all)\n",
    "g_all = np.hstack(g_all)\n",
    "pwr_all = np.hstack(pwr_all)\n",
    "plot.gradient_comparison(f_all, t_all, g_all, pwr_all, [ax1, ax2], alpha=0.1, cmap='Spectral')\n",
    "plot.gradient_comparison(z_all, t_all, g_all, pwr_all, [ax3, ax4], alpha=0.1, cmap='Spectral')\n",
    "plot.gradient_comparison(f_all, z_all, g_all, pwr_all, [ax5, ax6], alpha=0.1, cmap='Spectral')\n",
    "ax1.set_ylabel('all')\n",
    "l1 = ax1.set_xlabel('f vs t')\n",
    "l2 = ax3.set_xlabel('z vs t')\n",
    "l3 = ax5.set_xlabel('f vs z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like observations from the first run and values with power greater than about 0.95 do not behave well. Let's also assume that power less than 0.1 does not behave well either. So, we'll filter out the data from the lowest value and the power value s outside the limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 6)\n",
    "fig.set_size_inches(12, 10)\n",
    "f_all = []\n",
    "t_all = []\n",
    "z_all = []\n",
    "g_all = []\n",
    "pwr_all = []\n",
    "\n",
    "for (test_name, ax_r) in zip(*(tests, axes[1:])):\n",
    "#     print(test_name)\n",
    "    [ax1, ax2, ax3, ax4, ax5, ax6] = ax_r\n",
    "    data, nans = draw_traditional_values(test_name)\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.loc[(data['power'] < 0.95) & (data['power'] > 0.1) & (data['count'] > 5)]\n",
    "    f_ = data['f_effect']\n",
    "    t_ = data['t_effect']\n",
    "    z_ = data['z_effect']\n",
    "    g_ = data['count']\n",
    "    pwr = data['power']\n",
    "    \n",
    "    f_all.append(f_)\n",
    "    t_all.append(t_)\n",
    "    z_all.append(z_)\n",
    "    g_all.append(g_)\n",
    "    pwr_all.append(pwr)\n",
    "\n",
    "    plot.gradient_comparison(f_, t_, g_, pwr, [ax1, ax2], alpha=0.25, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "    plot.gradient_comparison(z_, t_, g_, pwr, [ax3, ax4], alpha=0.25, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "    plot.gradient_comparison(f_, z_, g_, pwr, [ax5, ax6], alpha=0.25, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "    ax1.set_xlabel('')\n",
    "    ax3.set_xlabel('')\n",
    "    ax5.set_xlabel('')\n",
    "    ax1.set_ylabel(test_name)\n",
    "    ax3.set_ylabel('')\n",
    "    ax5.set_ylabel('')\n",
    "\n",
    "sn.despine()\n",
    "\n",
    "[ax1, ax2, ax3, ax4, ax5, ax6] = axes[0]\n",
    "f_all = np.hstack(f_all)\n",
    "t_all = np.hstack(t_all)\n",
    "z_all = np.hstack(z_all)\n",
    "g_all = np.hstack(g_all)\n",
    "pwr_all = np.hstack(pwr_all)\n",
    "plot.gradient_comparison(f_all, t_all, g_all, pwr_all, [ax1, ax2], alpha=0.1, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "plot.gradient_comparison(z_all, t_all, g_all, pwr_all, [ax3, ax4], alpha=0.1, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "plot.gradient_comparison(f_all, z_all, g_all, pwr_all, [ax5, ax6], alpha=0.1, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "ax1.set_ylabel('all')\n",
    "l1 = ax1.set_title('f vs t')\n",
    "l2 = ax3.set_title('z vs t')\n",
    "l3 = ax5.set_title('f vs z')\n",
    "\n",
    "ax2.get_ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we exclude those values, it looks like we have, for the most part, a strong correlation between ($R^{2} \\geq 0.995$). However, we also find that there's a constant relationship between the T-test effect size and both the F and Z distribution. \n",
    "\n",
    "The Statsmodels effect is likely calculated as\n",
    "\n",
    "$\\begin{align}\n",
    "\\lambda &= \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s\\left (\\sqrt{1 / n} + \\sqrt{1 / n}\\right )}\\\\\n",
    "&= \\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s\\sqrt{2 \\ n}}\\\\\n",
    "&= \\sqrt{\\frac{n}{2}}\\left (\\frac{\\bar{x}_{1} - \\bar{x}_{2}}{s} \\right )\n",
    "\\end{align}$\n",
    "\n",
    "while the z test and f test implementations do not include this term. So, we can try to correct the effect sizes using this adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, 6)\n",
    "fig.set_size_inches(12, 10)\n",
    "f_all = []\n",
    "t_all = []\n",
    "z_all = []\n",
    "g_all = []\n",
    "pwr_all = []\n",
    "\n",
    "for (test_name, ax_r) in zip(*(tests, axes[1:])):\n",
    "#     print(test_name)\n",
    "    [ax1, ax2, ax3, ax4, ax5, ax6] = ax_r\n",
    "    data, nans = draw_traditional_values(test_name)\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.loc[(data['power'] < 0.95) & (data['power'] > 0.1) & (data['count'] > 5)]\n",
    "    f_ = data['f_effect']\n",
    "    t_ = data['t_effect'] / np.sqrt(2)\n",
    "    z_ = data['z_effect']\n",
    "    g_ = data['count']\n",
    "    pwr = data['power']\n",
    "    \n",
    "    f_all.append(f_)\n",
    "    t_all.append(t_)\n",
    "    z_all.append(z_)\n",
    "    g_all.append(g_)\n",
    "    pwr_all.append(pwr)\n",
    "\n",
    "    plot.gradient_comparison(f_, t_, g_, pwr, [ax1, ax2], alpha=0.25, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "    plot.gradient_comparison(z_, t_, g_, pwr, [ax3, ax4], alpha=0.25, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "    plot.gradient_comparison(f_, z_, g_, pwr, [ax5, ax6], alpha=0.25, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "    ax1.set_xlabel('')\n",
    "    ax3.set_xlabel('')\n",
    "    ax5.set_xlabel('')\n",
    "    ax1.set_ylabel(test_name)\n",
    "    ax3.set_ylabel('')\n",
    "    ax5.set_ylabel('')\n",
    "\n",
    "sn.despine()\n",
    "\n",
    "[ax1, ax2, ax3, ax4, ax5, ax6] = axes[0]\n",
    "f_all = np.hstack(f_all)\n",
    "t_all = np.hstack(t_all)\n",
    "z_all = np.hstack(z_all)\n",
    "g_all = np.hstack(g_all)\n",
    "pwr_all = np.hstack(pwr_all)\n",
    "plot.gradient_comparison(f_all, t_all, g_all, pwr_all, [ax1, ax2], alpha=0.1, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "plot.gradient_comparison(z_all, t_all, g_all, pwr_all, [ax3, ax4], alpha=0.1, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "plot.gradient_comparison(f_all, z_all, g_all, pwr_all, [ax5, ax6], alpha=0.1, cmap='Spectral', resid_y=[-0.1, 0.1])\n",
    "ax1.set_ylabel('all')\n",
    "l1 = ax1.set_title('f vs t')\n",
    "l2 = ax3.set_title('z vs t')\n",
    "l3 = ax5.set_title('f vs z')\n",
    "\n",
    "ax2.get_ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we see a slight difference in the shape of the curves, however, we see a high degree of correlations between the results.\n",
    "\n",
    "In the next notebook, we'll look at the way these effect sizes behave with the emperically calculated effects and emperically calculated power."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
